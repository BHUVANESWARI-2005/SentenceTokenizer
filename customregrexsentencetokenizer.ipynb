{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8143731,"sourceType":"datasetVersion","datasetId":4815310}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Custom Regrex Sentence Tokenizer","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport nltk\nnltk.download('punkt')\n\nfrom nltk.tokenize import sent_tokenize\n\n# Load dataset\n# Replace 'path_to_dataset.csv' with the actual path to the dataset containing text.\ndf = pd.read_csv(\"/kaggle/input/imdb-movie-reviews/IMDB Dataset.csv\")\n\n# Assume the dataset has a column named 'review' with textual data.\n# Inspect the dataset\nprint(df.head())\n\n# Using NLTK's sentence tokenizer\ndef nltk_sentence_tokenizer(text):\n    \"\"\"\n    Tokenize text into sentences using NLTK's sent_tokenize.\n    \"\"\"\n    return sent_tokenize(text)\n\n# Applying the NLTK sentence tokenizer to the dataset\ndf[\"sentences_nltk\"] = df[\"review\"].apply(nltk_sentence_tokenizer)\n\n# Using a Custom Regex-based Sentence Tokenizer\ndef regex_sentence_tokenizer(text):\n    \"\"\"\n    Tokenize text into sentences using regex.\n    Handles punctuation like '.', '!', '?', and special cases like abbreviations.\n    \"\"\"\n    # Regex pattern to split sentences\n    pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s'\n    return re.split(pattern, text)\n\n# Applying the custom regex-based tokenizer to the dataset\ndf[\"sentences_regex\"] = df[\"review\"].apply(regex_sentence_tokenizer)\n\n# Inspect the tokenized sentences\nprint(df[[\"review\", \"sentences_nltk\", \"sentences_regex\"]].head())\n\n# Save the tokenized sentences to a new CSV\ndf.to_csv(\"tokenized_sentences.csv\", index=False)\nprint(\"Tokenized sentences saved to 'tokenized_sentences.csv'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T16:23:40.361298Z","iopub.execute_input":"2025-01-09T16:23:40.361580Z","iopub.status.idle":"2025-01-09T16:24:11.334093Z","shell.execute_reply.started":"2025-01-09T16:23:40.361556Z","shell.execute_reply":"2025-01-09T16:24:11.332979Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n                                              review  \\\n0  One of the other reviewers has mentioned that ...   \n1  A wonderful little production. <br /><br />The...   \n2  I thought this was a wonderful way to spend ti...   \n3  Basically there's a family where a little boy ...   \n4  Petter Mattei's \"Love in the Time of Money\" is...   \n\n                                      sentences_nltk  \\\n0  [One of the other reviewers has mentioned that...   \n1  [A wonderful little production., <br /><br />T...   \n2  [I thought this was a wonderful way to spend t...   \n3  [Basically there's a family where a little boy...   \n4  [Petter Mattei's \"Love in the Time of Money\" i...   \n\n                                     sentences_regex  \n0  [One of the other reviewers has mentioned that...  \n1  [A wonderful little production., <br /><br />T...  \n2  [I thought this was a wonderful way to spend t...  \n3  [Basically there's a family where a little boy...  \n4  [Petter Mattei's \"Love in the Time of Money\" i...  \nTokenized sentences saved to 'tokenized_sentences.csv'.\n","output_type":"stream"}],"execution_count":1}]}